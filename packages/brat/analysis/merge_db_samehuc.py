"""
Merges standard BRAT databases (.gpkg) of the SAME REGION/HUC (e.g. with different models - FIS Sensitivity Analysis).
    For each reach, all columns to to copy will be collected ("lined up") in the new db.
    Assumptions:
        all rows (reaches) in each source database are equal and parallel.
        all columns to copy exist in each source database
    Result:
        # of items in the new db = # of items in any one source db (should be the same)

INSTRUCTIONS:
    Edit the config with your database paths and desired columns and just run the script.

Disclaimers:
    Script can only copy integer outputs; geometry columns will not be copied correctly.
    This script was partially generated by AI.

Evan Hackstadt
July 2025
"""

import os
import statistics
import sqlite3

# --- CONFIGURATION ---

# Paths to your source databases                               # SET THESE
source_dbs = {
    # 'path to db': 'shorthand label'
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Standard-FIS/outputs/brat.gpkg': 'Standard',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-SPlow-Left/outputs/brat.gpkg': 'SPL-10',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-SPlow-Right/outputs/brat.gpkg': 'SPL+10',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-SP2-Left/outputs/brat.gpkg': 'SP2-50',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-SP2-Right/outputs/brat.gpkg': 'SP2+50',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-Slope-Left/outputs/brat.gpkg': 'SLO-1',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shift-Slope-Right/outputs/brat.gpkg': 'SLO+1',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Veg-Compress/outputs/brat.gpkg': 'VEGx0.75',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Veg-Stretch/outputs/brat.gpkg': 'VEGx1.5',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Comb-Compress/outputs/brat.gpkg': 'HYDx0.75',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Comb-Stretch/outputs/brat.gpkg': 'HYDx1.5',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Both-Compress/outputs/brat.gpkg': 'BOTHx0.75',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Scale-Both-Stretch/outputs/brat.gpkg': 'BOTHx1.5',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shape-Veg/outputs/brat.gpkg': 'VEGcv2',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shape-Comb/outputs/brat.gpkg': 'HYDcv2',
    '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/Shape-Both/outputs/brat.gpkg': 'BOTHcv2'
}
# ReachIDs should be equal in all source dbs
# Shorthand label will be used in the columns of the merged db

# Name of the table to extract from in each source database
source_table = 'ReachAttributes'

# HUC that all source_dbs share
huc = 1710020407

# Columns to copy (must exist in all source tables)
columns_to_copy_once = [    # independent columns
    'ReachID',  # do not change
    'WatershedID'
]
columns_to_copy_each = [    # from each source db
    'oVC_EX',
    'oCC_EX'
]

# Name of the new database and table
new_db_name = 'brat-all-fis.db'
new_db_dir = '/Users/evan/Code/OSU-EB3-REU/sqlBRAT/fis-runs/Lower-Siletz-River-1710020407/ALL'  # SET THIS
new_table_name = 'CombinedOutputs'

# Useful supplemental tables.
stats_table = True      # summarizes mean, st.dev, etc. of each column for all reaches
adjustments_table = True    # summarizes the FIS adjustments made in each source db



# --- SCRIPT STARTS HERE ---

new_db_path = os.path.join(new_db_dir, new_db_name)

with sqlite3.connect(new_db_path) as conn:
    cur = conn.cursor()

    # Useful row and column lists and strings
    ind_cols = columns_to_copy_once
    ind_col_stmt = ', '.join(ind_cols)
    src_dep_col_stmt = ', '.join(columns_to_copy_each)

    new_dep_cols = []
    for label in source_dbs.values():
        for col in columns_to_copy_each:
            new_dep_cols.append(f"{col}_{label}")

    new_all_cols = ind_cols + new_dep_cols
    new_all_cols_stmt = ', '.join(new_all_cols)

    # Create main table
    cur.execute(f"DROP TABLE IF EXISTS {new_table_name}")
    cur.execute(f"CREATE TABLE {new_table_name} ({new_all_cols_stmt})")
    conn.commit()

    # Populate new db. Gather relevant data and store

    # First get independent col data once from first source db
    db = list(source_dbs.keys())[0]
    print(f"Querying independent data from {db}...")
    src_conn = sqlite3.connect(db)
    src_cur = src_conn.cursor()
    src_cur.execute(f"SELECT {ind_col_stmt} FROM {source_table}")
    ind_rows = src_cur.fetchall()
    data = [list(row) for row in ind_rows]  # convert from tuple

    # Now, for each source db, get data for all reaches and store
    for db, label in source_dbs.items():
        print(f"Now processing {db}...")
        src_conn = sqlite3.connect(db)
        src_cur = src_conn.cursor()
        src_cur.execute(f"SELECT {src_dep_col_stmt} FROM {source_table}")
        dep_rows = src_cur.fetchall()
        for i in range(len(data)):    # assuming db structures parallel
            data[i] += list(dep_rows[i])    # append this db's dep cols
        src_conn.close()
    # Insert stored data
    placeholders = ', '.join(['?'] * (len(data[0])))
    print(placeholders)
    insert_stmt = f"INSERT INTO {new_table_name} ({new_all_cols_stmt}) VALUES ({placeholders})"
    print(insert_stmt)
    cur.executemany(insert_stmt, data)

    conn.commit()
    print(f"Inserted {len(ind_rows)} rows of data for all columns")


    # —— Populate stats table if requested ——
    if stats_table:
        print(f"Processing all reaches into Stats table...")

        categories = ['None', 'Rare', 'Occasional', 'Frequent', 'Pervasive']

        cap_cutoffs = [
            {'label': categories[0], 'upper': 0},
            {'label': categories[1], 'lower': 0, 'upper': 1},
            {'label': categories[2], 'lower': 1, 'upper': 5},
            {'label': categories[3], 'lower': 5, 'upper': 15},
            {'label': categories[4], 'lower': 15}
        ]
        
        stat_cols = ["Mean", "St_Dev", "Min", "Max"]
        stat_cols += [f'{cat}_Percent' for cat in categories]       # currently: only oCC % data
        cur.execute("DROP TABLE IF EXISTS Stats")
        cur.execute(f"CREATE TABLE Stats (Label TEXT, {', '.join(stat_cols)})")
        
        results = {stat: [] for stat in stat_cols}    # Mean: [means for each data source], ...
        # compute stats for each data source
        for col in new_dep_cols:
            cur.execute(f"SELECT AVG({col}), MIN({col}), MAX({col}) FROM {new_table_name}")
            mean_, min_, max_ = cur.fetchone()
            # store in dictionary
            results["Mean"].append(round(mean_, 3))
            results["Min"].append(round(min_, 2))
            results["Max"].append(round(max_, 2))

            # use python for st.dev
            cur.execute(f"SELECT {col} FROM {new_table_name}")
            values = [row[0] for row in cur.fetchall() if row[0] is not None]
            if len(values) > 1:
                stdev = round(statistics.stdev(values), 3)
            else:
                stdev = None
            results["St_Dev"].append(stdev)

            # compute category % stats for this col
            for src_var in columns_to_copy_each:
                if src_var in col:
                    col_var = src_var
                    break

            # Now calculate % of each capacity categories — currently only if oCC column
            # Code adapted from Riverscapes' brat_report.py
            # % of reaches in category = total length of reaches with oCC_EX in bounds / total length of all reaches
            if 'oCC_EX' in col:
                # Find corresponding source db so we can pull data from there
                col_label = col.split("_")[-1]
                for path, label in source_dbs.items():
                    if label == col_label:
                        db_path = path
                        break
                src_conn = sqlite3.connect(db_path)
                src_cur = src_conn.cursor()
                
                for cat_dict in cap_cutoffs:
                    label = cat_dict['label']
                    lower = cat_dict['lower'] if 'lower' in cat_dict else None
                    upper = cat_dict['upper'] if 'upper' in cat_dict else None
                    extra_clauses = []
                    extra_args = []
                    if lower is not None:
                        extra_clauses.append('r.oCC_EX > ?')
                        extra_args.append(lower)
                    if upper is not None:
                        extra_clauses.append('r.oCC_EX <= ?')
                        extra_args.append(upper)

                    # Build the WHERE clause
                    where_sql = 'r.WatershedID = ?'
                    if extra_clauses:
                        where_sql += ' AND ' + ' AND '.join(extra_clauses)

                    # Use JOIN for denominator clarity
                    src_cur.execute(f"""
                        SELECT (0.1 * SUM(r.iGeo_Len) / t.total_length) AS Percent
                        FROM {source_table} r
                        JOIN (
                            SELECT SUM(iGeo_Len) / 1000 AS total_length
                            FROM {source_table}
                            WHERE WatershedID = ?
                        ) t ON 1=1
                        WHERE {where_sql}
                    """, [huc, huc] + extra_args)
                    row = src_cur.fetchone()
                    percent = round(row[0], 2) if row and row[0] is not None else None
                    results[f'{cat_dict["label"]}_Percent'].append(percent)
                print(f"Calculated oCC_EX percents for {col}")
            else:
                for cat_dict in cap_cutoffs:
                    results[f'{cat_dict["label"]}_Percent'].append(None)

        # insert a row for each dependent column (source data)
        # stats are columns
        placeholders = ', '.join(['?'] * (1 + len(stat_cols)))
        for i in range(len(new_dep_cols)):
            row = []
            row.append(new_dep_cols[i])     # label
            for stat in stat_cols:
                row.append(results[stat][i])
            cur.execute(f"INSERT INTO Stats VALUES ({placeholders})", row)
        
        conn.commit()
        print(f"Stats table populated successfully.")
        
        
    # ——— Populate adj table if requested ———
    if adjustments_table:
        print(f"Summarizing source databases into Adjustments Table...")
        
        adj_cols = ["Veg_Type", "Veg_Val", "Comb_Type", "Comb_SPlow_Val", "Comb_SP2_Val", "Comb_Slope_Val"]
        cur.execute("DROP TABLE IF EXISTS Adjustments")
        cur.execute(f"CREATE TABLE Adjustments (Label, {', '.join(adj_cols)})")
        
        adj_data = {label: {col: None for col in adj_cols} for label in new_dep_cols}   # oVC_EX_ST: [Veg_Type, Veg_Val, Comb_Type, ...], ...
        # get adjustment data from each source db
        for db in source_dbs:
            with sqlite3.connect(db) as src_conn:
                src_cur = src_conn.cursor()
                # know which labels correspond to this db
                db_shorthand = source_dbs[db]
                db_labels = [col for col in new_dep_cols if db_shorthand in col]
                # check for source FIS_Adjustments table
                src_cur.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='FIS_Adjustments'")
                fis_table = src_cur.fetchall()
                if fis_table == []:
                    print(f"FIS table not found for database {db}. Inserting NULLs...")
                    continue    # skip this db
                # assuming table exists, get data
                src_cur.execute(f"SELECT FIS, MF, Adj_Type, Adj_Value FROM FIS_Adjustments")
                db_fis_data = src_cur.fetchall()    # [(FIS, MF, Adj_Type, Adj_Value), (FIS, ...), ...]

                # extract FIS data and store it for the appropriate labels
                for row in db_fis_data:
                    if row[0] == "Vegetation FIS":
                        for label in db_labels:
                            if row[2] is not None:  # only store if non-Null Adj_Type
                                adj_data[label]["Veg_Type"] = row[2]
                                adj_data[label]["Veg_Val"] = row[3]
                    elif row[0] == "Combined FIS":
                        for label in db_labels:
                            if row[2] is not None:
                                adj_data[label]["Comb_Type"] = row[2]
                                if row[1] == "SPlow": adj_data[label]["Comb_SPlow_Val"] = row[3]
                                if row[1] == "SP2": adj_data[label]["Comb_SP2_Val"] = row[3]
                                if row[1] == "Slope": adj_data[label]["Comb_Slope_Val"] = row[3]
            
        # now insert all data for all dbs
        # rows are source data labels (main table dep columns)
        # we will have redundancy but the label rows will match the stats table and main table cols
        placeholders = ', '.join(['?'] * (1 + len(adj_cols)))
        for label, data in adj_data.items():
            insert_stmt = f"INSERT INTO Adjustments VALUES ({placeholders})"
            row = [label] + [data.get(val) for val in adj_cols]
            cur.execute(insert_stmt, row)

        conn.commit()
        print(f"Adjustments table populated successfully.")


print("Merging complete! Data is in", new_db_path)